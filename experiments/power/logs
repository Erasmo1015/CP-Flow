2024-03-22 22:26:22,859 - Namespace(data='power', arch='icnn2', softplus_type='softplus', zero_softplus=False, symm_act_first=False, trainable_w0=True, clip_grad=0, preload_data=False, dimh=512, nhidden=5, nblocks=10, nepochs=1000, early_stopping=20, batch_size=1024, val_batch_size=1024, test_batch_size=1024, lr=0.001, wd=1e-06, atol=0.001, rtol=0.0, cuda=None, seed=0, data_root='data/', resume=None, save='experiments/power', evaluate=False, fp64=False, val_freq=500, brute_val=False, log_freq=100, train_est_freq=100)
2024-03-22 22:26:22,900 - Using GPU: None of the 2
2024-03-22 22:26:25,882 - SequentialFlow(
  (flows): ModuleList(
    (0): ActNorm(6)
    (1): DeepConvexFlow(
      ConjGrad(rtol=0.0, atol=0.001)
      (icnn): ICNN2(
        (act): Softplus()
        (Wzs): ModuleList(
          (0): Linear(in_features=6, out_features=512, bias=True)
          (1-4): 4 x PosLinear(in_features=512, out_features=512, bias=True)
          (5): PosLinear(in_features=512, out_features=1, bias=False)
        )
        (Wxs): ModuleList(
          (0-3): 4 x Linear(in_features=6, out_features=512, bias=True)
          (4): Linear(in_features=6, out_features=1, bias=False)
        )
        (actnorm0): ActNormNoLogdet(512)
        (actnorms): ModuleList(
          (0-3): 4 x ActNormNoLogdet(512)
          (4): ActNormNoLogdet(1)
        )
      )
    )
    (2): ActNorm(6)
    (3): DeepConvexFlow(
      ConjGrad(rtol=0.0, atol=0.001)
      (icnn): ICNN2(
        (act): Softplus()
        (Wzs): ModuleList(
          (0): Linear(in_features=6, out_features=512, bias=True)
          (1-4): 4 x PosLinear(in_features=512, out_features=512, bias=True)
          (5): PosLinear(in_features=512, out_features=1, bias=False)
        )
        (Wxs): ModuleList(
          (0-3): 4 x Linear(in_features=6, out_features=512, bias=True)
          (4): Linear(in_features=6, out_features=1, bias=False)
        )
        (actnorm0): ActNormNoLogdet(512)
        (actnorms): ModuleList(
          (0-3): 4 x ActNormNoLogdet(512)
          (4): ActNormNoLogdet(1)
        )
      )
    )
    (4): ActNorm(6)
    (5): DeepConvexFlow(
      ConjGrad(rtol=0.0, atol=0.001)
      (icnn): ICNN2(
        (act): Softplus()
        (Wzs): ModuleList(
          (0): Linear(in_features=6, out_features=512, bias=True)
          (1-4): 4 x PosLinear(in_features=512, out_features=512, bias=True)
          (5): PosLinear(in_features=512, out_features=1, bias=False)
        )
        (Wxs): ModuleList(
          (0-3): 4 x Linear(in_features=6, out_features=512, bias=True)
          (4): Linear(in_features=6, out_features=1, bias=False)
        )
        (actnorm0): ActNormNoLogdet(512)
        (actnorms): ModuleList(
          (0-3): 4 x ActNormNoLogdet(512)
          (4): ActNormNoLogdet(1)
        )
      )
    )
    (6): ActNorm(6)
    (7): DeepConvexFlow(
      ConjGrad(rtol=0.0, atol=0.001)
      (icnn): ICNN2(
        (act): Softplus()
        (Wzs): ModuleList(
          (0): Linear(in_features=6, out_features=512, bias=True)
          (1-4): 4 x PosLinear(in_features=512, out_features=512, bias=True)
          (5): PosLinear(in_features=512, out_features=1, bias=False)
        )
        (Wxs): ModuleList(
          (0-3): 4 x Linear(in_features=6, out_features=512, bias=True)
          (4): Linear(in_features=6, out_features=1, bias=False)
        )
        (actnorm0): ActNormNoLogdet(512)
        (actnorms): ModuleList(
          (0-3): 4 x ActNormNoLogdet(512)
          (4): ActNormNoLogdet(1)
        )
      )
    )
    (8): ActNorm(6)
    (9): DeepConvexFlow(
      ConjGrad(rtol=0.0, atol=0.001)
      (icnn): ICNN2(
        (act): Softplus()
        (Wzs): ModuleList(
          (0): Linear(in_features=6, out_features=512, bias=True)
          (1-4): 4 x PosLinear(in_features=512, out_features=512, bias=True)
          (5): PosLinear(in_features=512, out_features=1, bias=False)
        )
        (Wxs): ModuleList(
          (0-3): 4 x Linear(in_features=6, out_features=512, bias=True)
          (4): Linear(in_features=6, out_features=1, bias=False)
        )
        (actnorm0): ActNormNoLogdet(512)
        (actnorms): ModuleList(
          (0-3): 4 x ActNormNoLogdet(512)
          (4): ActNormNoLogdet(1)
        )
      )
    )
    (10): ActNorm(6)
    (11): DeepConvexFlow(
      ConjGrad(rtol=0.0, atol=0.001)
      (icnn): ICNN2(
        (act): Softplus()
        (Wzs): ModuleList(
          (0): Linear(in_features=6, out_features=512, bias=True)
          (1-4): 4 x PosLinear(in_features=512, out_features=512, bias=True)
          (5): PosLinear(in_features=512, out_features=1, bias=False)
        )
        (Wxs): ModuleList(
          (0-3): 4 x Linear(in_features=6, out_features=512, bias=True)
          (4): Linear(in_features=6, out_features=1, bias=False)
        )
        (actnorm0): ActNormNoLogdet(512)
        (actnorms): ModuleList(
          (0-3): 4 x ActNormNoLogdet(512)
          (4): ActNormNoLogdet(1)
        )
      )
    )
    (12): ActNorm(6)
    (13): DeepConvexFlow(
      ConjGrad(rtol=0.0, atol=0.001)
      (icnn): ICNN2(
        (act): Softplus()
        (Wzs): ModuleList(
          (0): Linear(in_features=6, out_features=512, bias=True)
          (1-4): 4 x PosLinear(in_features=512, out_features=512, bias=True)
          (5): PosLinear(in_features=512, out_features=1, bias=False)
        )
        (Wxs): ModuleList(
          (0-3): 4 x Linear(in_features=6, out_features=512, bias=True)
          (4): Linear(in_features=6, out_features=1, bias=False)
        )
        (actnorm0): ActNormNoLogdet(512)
        (actnorms): ModuleList(
          (0-3): 4 x ActNormNoLogdet(512)
          (4): ActNormNoLogdet(1)
        )
      )
    )
    (14): ActNorm(6)
    (15): DeepConvexFlow(
      ConjGrad(rtol=0.0, atol=0.001)
      (icnn): ICNN2(
        (act): Softplus()
        (Wzs): ModuleList(
          (0): Linear(in_features=6, out_features=512, bias=True)
          (1-4): 4 x PosLinear(in_features=512, out_features=512, bias=True)
          (5): PosLinear(in_features=512, out_features=1, bias=False)
        )
        (Wxs): ModuleList(
          (0-3): 4 x Linear(in_features=6, out_features=512, bias=True)
          (4): Linear(in_features=6, out_features=1, bias=False)
        )
        (actnorm0): ActNormNoLogdet(512)
        (actnorms): ModuleList(
          (0-3): 4 x ActNormNoLogdet(512)
          (4): ActNormNoLogdet(1)
        )
      )
    )
    (16): ActNorm(6)
    (17): DeepConvexFlow(
      ConjGrad(rtol=0.0, atol=0.001)
      (icnn): ICNN2(
        (act): Softplus()
        (Wzs): ModuleList(
          (0): Linear(in_features=6, out_features=512, bias=True)
          (1-4): 4 x PosLinear(in_features=512, out_features=512, bias=True)
          (5): PosLinear(in_features=512, out_features=1, bias=False)
        )
        (Wxs): ModuleList(
          (0-3): 4 x Linear(in_features=6, out_features=512, bias=True)
          (4): Linear(in_features=6, out_features=1, bias=False)
        )
        (actnorm0): ActNormNoLogdet(512)
        (actnorms): ModuleList(
          (0-3): 4 x ActNormNoLogdet(512)
          (4): ActNormNoLogdet(1)
        )
      )
    )
    (18): ActNorm(6)
    (19): DeepConvexFlow(
      ConjGrad(rtol=0.0, atol=0.001)
      (icnn): ICNN2(
        (act): Softplus()
        (Wzs): ModuleList(
          (0): Linear(in_features=6, out_features=512, bias=True)
          (1-4): 4 x PosLinear(in_features=512, out_features=512, bias=True)
          (5): PosLinear(in_features=512, out_features=1, bias=False)
        )
        (Wxs): ModuleList(
          (0-3): 4 x Linear(in_features=6, out_features=512, bias=True)
          (4): Linear(in_features=6, out_features=1, bias=False)
        )
        (actnorm0): ActNormNoLogdet(512)
        (actnorms): ModuleList(
          (0-3): 4 x ActNormNoLogdet(512)
          (4): ActNormNoLogdet(1)
        )
      )
    )
    (20): ActNorm(6)
  )
)
2024-03-22 22:26:25,884 - Number of trainable parameters:10741992
2024-03-22 22:26:29,203 - Iter 000000 | Epoch 0.00 | Time 3.142 | GradNorm 0.79 | CG iters 20 (20.00) | Train logp -7.766046 (-7.766046)
2024-03-22 22:29:46,043 - [VAL] Iter 000000 | Val logp -7.626729 | NoImproveEpochs 00/20
2024-03-22 22:31:52,619 - Iter 000100 | Epoch 0.06 | Time 2.171 | GradNorm 2.82 | CG iters 30 (26.62) | Train logp -3.864719 (-4.382112)
2024-03-22 22:35:10,667 - Iter 000200 | Epoch 0.12 | Time 2.248 | GradNorm 3.83 | CG iters 35 (33.49) | Train logp -2.585568 (-2.823825)
2024-03-22 22:37:31,278 - Iter 000300 | Epoch 0.19 | Time 2.266 | GradNorm 4.78 | CG iters 36 (35.67) | Train logp -2.438559 (-2.489652)
2024-03-22 22:40:47,388 - Iter 000400 | Epoch 0.25 | Time 2.299 | GradNorm 4.95 | CG iters 38 (37.15) | Train logp -1.951112 (-2.022533)
2024-03-22 22:43:11,467 - Iter 000500 | Epoch 0.31 | Time 2.348 | GradNorm 5.45 | CG iters 38 (37.89) | Train logp -1.296700 (-1.392960)
2024-03-22 22:46:27,869 - [VAL] Iter 000500 | Val logp -1.641768 | NoImproveEpochs 00/20
2024-03-22 22:49:20,485 - Iter 000600 | Epoch 0.37 | Time 2.403 | GradNorm 7.62 | CG iters 38 (38.05) | Train logp -1.441748 (-1.435277)
2024-03-22 22:52:05,384 - Iter 000700 | Epoch 0.43 | Time 2.325 | GradNorm 6.27 | CG iters 40 (38.96) | Train logp -1.115044 (-1.157513)
2024-03-22 22:55:25,585 - Iter 000800 | Epoch 0.49 | Time 2.356 | GradNorm 7.52 | CG iters 42 (41.08) | Train logp -1.329665 (-1.306834)
2024-03-22 22:57:56,164 - Iter 000900 | Epoch 0.56 | Time 2.373 | GradNorm 7.63 | CG iters 42 (41.95) | Train logp -0.990228 (-1.032216)
2024-03-22 23:01:29,442 - Iter 001000 | Epoch 0.62 | Time 2.383 | GradNorm 8.18 | CG iters 43 (42.27) | Train logp -0.793407 (-0.825077)
2024-03-22 23:03:52,584 - [VAL] Iter 001000 | Val logp -0.789460 | NoImproveEpochs 00/20
2024-03-22 23:07:13,894 - Iter 001100 | Epoch 0.68 | Time 2.377 | GradNorm 7.49 | CG iters 43 (42.73) | Train logp -0.799335 (-0.802749)
2024-03-22 23:10:08,556 - Iter 001200 | Epoch 0.74 | Time 5.980 | GradNorm 10.05 | CG iters 43 (42.98) | Train logp -0.699593 (-0.713274)
2024-03-22 23:13:29,851 - Iter 001300 | Epoch 0.80 | Time 3.572 | GradNorm 9.57 | CG iters 43 (43.14) | Train logp -0.637751 (-0.647767)
2024-03-22 23:16:20,258 - Iter 001400 | Epoch 0.86 | Time 3.416 | GradNorm 12.11 | CG iters 44 (43.53) | Train logp -0.706293 (-0.698531)
2024-03-22 23:19:25,529 - Iter 001500 | Epoch 0.93 | Time 2.499 | GradNorm 11.86 | CG iters 43 (43.81) | Train logp -0.247793 (-0.307570)
2024-03-22 23:22:11,010 - [VAL] Iter 001500 | Val logp -0.455966 | NoImproveEpochs 00/20
2024-03-22 23:25:07,542 - Iter 001600 | Epoch 0.99 | Time 2.426 | GradNorm 13.66 | CG iters 44 (43.97) | Train logp -0.546823 (-0.515093)
2024-03-22 23:28:24,429 - Iter 001700 | Epoch 1.05 | Time 3.552 | GradNorm 12.72 | CG iters 44 (43.98) | Train logp -0.318579 (-0.344640)
2024-03-22 23:31:02,070 - Iter 001800 | Epoch 1.11 | Time 2.410 | GradNorm 13.49 | CG iters 44 (44.00) | Train logp -0.474258 (-0.457068)
2024-03-22 23:34:20,432 - Iter 001900 | Epoch 1.17 | Time 2.409 | GradNorm 12.26 | CG iters 44 (44.00) | Train logp -0.325965 (-0.343352)
2024-03-22 23:36:59,026 - Iter 002000 | Epoch 1.23 | Time 2.419 | GradNorm 13.23 | CG iters 44 (44.00) | Train logp -0.352412 (-0.351210)
2024-03-22 23:40:17,117 - [VAL] Iter 002000 | Val logp -0.296300 | NoImproveEpochs 00/20
2024-03-22 23:43:31,886 - Iter 002100 | Epoch 1.30 | Time 3.510 | GradNorm 14.42 | CG iters 44 (44.00) | Train logp -0.216450 (-0.234322)
2024-03-22 23:46:22,859 - Iter 002200 | Epoch 1.36 | Time 3.451 | GradNorm 14.62 | CG iters 44 (44.00) | Train logp -0.179386 (-0.186671)
2024-03-22 23:49:32,963 - Iter 002300 | Epoch 1.42 | Time 2.414 | GradNorm 14.71 | CG iters 44 (44.00) | Train logp -0.507638 (-0.465072)
2024-03-22 23:53:06,904 - Iter 002400 | Epoch 1.48 | Time 3.280 | GradNorm 14.29 | CG iters 44 (44.00) | Train logp -0.095432 (-0.144454)
2024-03-22 23:57:22,142 - Iter 002500 | Epoch 1.54 | Time 6.551 | GradNorm 14.89 | CG iters 44 (44.00) | Train logp -0.257946 (-0.242895)
2024-03-22 23:59:50,984 - [VAL] Iter 002500 | Val logp -0.212516 | NoImproveEpochs 00/20
2024-03-23 00:03:23,958 - Iter 002600 | Epoch 1.60 | Time 2.603 | GradNorm 13.46 | CG iters 44 (43.99) | Train logp -0.181035 (-0.189239)
2024-03-23 00:06:05,562 - Iter 002700 | Epoch 1.67 | Time 2.420 | GradNorm 15.36 | CG iters 44 (43.99) | Train logp -0.309319 (-0.293394)
2024-03-23 00:09:29,285 - Iter 002800 | Epoch 1.73 | Time 2.548 | GradNorm 14.19 | CG iters 44 (43.91) | Train logp -0.164694 (-0.181762)
2024-03-23 00:12:32,348 - Iter 002900 | Epoch 1.79 | Time 2.408 | GradNorm 16.08 | CG iters 44 (43.93) | Train logp 0.007690 (-0.017435)
2024-03-23 00:15:38,722 - Iter 003000 | Epoch 1.85 | Time 2.536 | GradNorm 14.10 | CG iters 44 (43.90) | Train logp -0.125855 (-0.111477)
2024-03-23 00:18:44,247 - [VAL] Iter 003000 | Val logp -0.110569 | NoImproveEpochs 00/20
2024-03-23 00:21:39,803 - Iter 003100 | Epoch 1.91 | Time 3.461 | GradNorm 15.77 | CG iters 44 (43.79) | Train logp -0.296571 (-0.272024)
2024-03-23 00:24:47,742 - Iter 003200 | Epoch 1.97 | Time 2.407 | GradNorm 17.59 | CG iters 44 (43.75) | Train logp -0.165020 (-0.179211)
2024-03-23 00:28:12,618 - Iter 003300 | Epoch 2.04 | Time 2.375 | GradNorm 14.96 | CG iters 43 (43.46) | Train logp 0.122444 (0.082439)
2024-03-23 00:30:54,441 - Iter 003400 | Epoch 2.10 | Time 2.380 | GradNorm 15.31 | CG iters 43 (43.38) | Train logp -0.106206 (-0.081188)
2024-03-23 00:34:28,411 - Iter 003500 | Epoch 2.16 | Time 3.558 | GradNorm 16.62 | CG iters 43 (43.22) | Train logp -0.011228 (-0.020506)
2024-03-23 00:37:09,863 - [VAL] Iter 003500 | Val logp -0.084172 | NoImproveEpochs 00/20
2024-03-23 00:40:29,079 - Iter 003600 | Epoch 2.22 | Time 2.393 | GradNorm 17.49 | CG iters 43 (43.24) | Train logp -0.192635 (-0.169807)
2024-03-23 00:43:32,004 - Iter 003700 | Epoch 2.28 | Time 3.459 | GradNorm 17.80 | CG iters 43 (43.13) | Train logp 0.089767 (0.055342)
2024-03-23 00:46:35,429 - Iter 003800 | Epoch 2.34 | Time 2.400 | GradNorm 17.66 | CG iters 43 (43.11) | Train logp -0.049637 (-0.035715)
2024-03-23 00:49:58,015 - Iter 003900 | Epoch 2.41 | Time 2.383 | GradNorm 16.27 | CG iters 43 (43.06) | Train logp 0.015593 (0.008789)
2024-03-23 00:52:42,621 - Iter 004000 | Epoch 2.47 | Time 2.654 | GradNorm 15.25 | CG iters 43 (43.11) | Train logp 0.052357 (0.046579)
2024-03-23 00:55:50,266 - [VAL] Iter 004000 | Val logp 0.032077 | NoImproveEpochs 00/20
2024-03-23 00:59:06,105 - Iter 004100 | Epoch 2.53 | Time 8.540 | GradNorm 17.68 | CG iters 43 (43.08) | Train logp -0.000547 (0.005703)
2024-03-23 01:02:05,415 - Iter 004200 | Epoch 2.59 | Time 2.405 | GradNorm 17.57 | CG iters 44 (43.06) | Train logp 0.151161 (0.131871)
2024-03-23 01:05:46,421 - Iter 004300 | Epoch 2.65 | Time 3.005 | GradNorm 18.48 | CG iters 43 (43.10) | Train logp -0.283726 (-0.228609)
2024-03-23 01:09:00,112 - Iter 004400 | Epoch 2.71 | Time 2.395 | GradNorm 15.06 | CG iters 43 (43.10) | Train logp -0.010028 (-0.039016)
2024-03-23 01:12:05,068 - Iter 004500 | Epoch 2.78 | Time 2.398 | GradNorm 17.52 | CG iters 43 (43.12) | Train logp 0.121513 (0.100223)
2024-03-23 01:15:25,044 - [VAL] Iter 004500 | Val logp 0.045770 | NoImproveEpochs 00/20
2024-03-23 01:19:04,464 - Iter 004600 | Epoch 2.84 | Time 2.390 | GradNorm 20.90 | CG iters 43 (43.12) | Train logp -0.158608 (-0.124282)
2024-03-23 01:21:51,819 - Iter 004700 | Epoch 2.90 | Time 2.393 | GradNorm 17.06 | CG iters 43 (43.19) | Train logp 0.029048 (0.008713)
2024-03-23 01:25:18,640 - Iter 004800 | Epoch 2.96 | Time 2.405 | GradNorm 17.75 | CG iters 43 (43.30) | Train logp 0.104646 (0.091923)
2024-03-23 01:28:23,695 - Iter 004900 | Epoch 3.02 | Time 2.419 | GradNorm 19.91 | CG iters 43 (43.01) | Train logp 0.178553 (0.167064)
2024-03-23 01:31:30,346 - Iter 005000 | Epoch 3.08 | Time 2.400 | GradNorm 17.27 | CG iters 43 (43.13) | Train logp 0.039510 (0.056426)
2024-03-23 01:34:39,267 - [VAL] Iter 005000 | Val logp 0.069178 | NoImproveEpochs 00/20
2024-03-23 01:37:40,408 - Iter 005100 | Epoch 3.15 | Time 3.312 | GradNorm 17.55 | CG iters 43 (43.17) | Train logp -0.000304 (0.007220)
2024-03-23 01:40:52,476 - Iter 005200 | Epoch 3.21 | Time 2.397 | GradNorm 17.17 | CG iters 42 (43.19) | Train logp 0.116625 (0.102116)
2024-03-23 01:44:10,110 - Iter 005300 | Epoch 3.27 | Time 3.458 | GradNorm 17.82 | CG iters 43 (43.22) | Train logp -0.053808 (-0.033130)
2024-03-23 01:47:20,763 - Iter 005400 | Epoch 3.33 | Time 3.703 | GradNorm 17.62 | CG iters 42 (43.24) | Train logp 0.388492 (0.332577)
2024-03-23 01:50:36,762 - Iter 005500 | Epoch 3.39 | Time 3.066 | GradNorm 19.02 | CG iters 44 (43.00) | Train logp 0.150015 (0.174226)
2024-03-23 01:54:12,022 - [VAL] Iter 005500 | Val logp 0.069141 | NoImproveEpochs 01/20
2024-03-23 01:57:20,454 - Iter 005600 | Epoch 3.45 | Time 2.482 | GradNorm 17.76 | CG iters 43 (42.94) | Train logp 0.008267 (0.030276)
2024-03-23 02:00:48,616 - Iter 005700 | Epoch 3.52 | Time 3.791 | GradNorm 20.64 | CG iters 42 (42.99) | Train logp 0.378027 (0.331909)
2024-03-23 02:03:59,967 - Iter 005800 | Epoch 3.58 | Time 2.461 | GradNorm 19.98 | CG iters 42 (42.92) | Train logp 0.060513 (0.096505)
2024-03-23 02:07:10,595 - Iter 005900 | Epoch 3.64 | Time 2.943 | GradNorm 20.20 | CG iters 43 (43.11) | Train logp -0.046663 (-0.027676)
2024-03-23 02:10:39,773 - Iter 006000 | Epoch 3.70 | Time 2.560 | GradNorm 18.23 | CG iters 43 (42.98) | Train logp 0.227549 (0.193701)
2024-03-23 02:13:16,809 - [VAL] Iter 006000 | Val logp 0.134645 | NoImproveEpochs 00/20
2024-03-23 02:16:47,363 - Iter 006100 | Epoch 3.76 | Time 2.489 | GradNorm 17.99 | CG iters 42 (43.13) | Train logp 0.138192 (0.145554)
